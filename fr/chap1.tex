\chapter{Historique et Définition} \label{1}

Selon Paul Ceruzzi, il y a deux courants d'histoire qui se sont construits autour de l'objet logiciel \citep{Ceruzzi1998}. Le premier de ces courants, qui évite une description trop technique pour être destinée à un large public, s'intéresse à la formation des compagnies privées individuelles (IBM, Apple, Microsoft). Le seconde historiographie, elle, se concentre sur la naissance des langages de programmation (FORTRAN, COBOL, entre autres), en questionnant comment chaque langage est apparu et a permis à ses développeurs d'extraire de nouvelles fonctionnalités du matériel.

Depuis les travaux de Ceruzzi, un troisième courant d'historiographie s'est construit petit à petit autour de l'observation du mouvement du logiciel libre durant les années 1980 et 1990. Cette nouvelle narrative du mouvement technologique informatique rend compte des traditions de la culture hacker de programmation. Depuis ses origines dans les laboratoires universitaires d'informatique dans les années 1950 et 1960, s'est contruit l'histoire de comment des facteurs économiques et idéologiques ont formé les traditions culturelles de la programmation et comment ces récentes traditions ont affecté et même déterminé, quel type de logiciel est en train d'être écrit.

Ce chapitre introductif a pour objectif d'utiliser librement ces trois courants d'historiographie de l'informatique pour présenter le mouvement du logiciel libre dans son contexte historique, social et économique. Dans un premier temps, nous montrons comment le concept de logiciel s'est formé comme une abstraction du matériel et nous décrivons les éléments concrets que recouvrent, aujourd'hui, ces deux notions (\ref{1.1}). Ensuite, nous expliquons comment l'objet logiciel s'est construit à grande échelle, en accompagnant l'histoire de l'ordinateur personnel et des compagnies qui l'ont fabriqué. Ainsi, on peut comprendre les premières traçes de la culture \emph{open-source} comme une réaction à la privatisation du code (\ref{1.2}). De plus, dans la troisième partie, nous montrerons la constitution des communautés et licences qui forment le mouvement, ainsi que les institutions qui le promeuvent. De cette manière, à travers les conflits d'interprétation sur ce qu'est être "libre" ou "ouvert" par rapport à une technologie, on observe l'hétérogénéité du mouvement qui appelle  à postuler d'une définition systématique (\ref{1.3}). Enfin, dans la quatrième et dernière section, nous présenterons quelques uns des défis contemporains qui sont en débat au sein des communautés, afin d'illustrer les succès et carences propres au mouvement (\ref{1.4}).

\section{Ordinateur, matériel et logiciel} \label{1.1}

\subsection{L'abstraction du logiciel} \label{1.1.1}

L'origine du mot "\emph{computer}" désigne un homme réalisant des opérations mathématiques avec l'aide d'outils mécaniques. Parmi ces outils qui restent les plus célèbres, on trouve le boulier (2700-­2300 av J.­C.), le  mécanisme antikythera (150­-100 av J.­C.), la règle de calcul et l'astrobale datant de la Renaissance. Les  premiers   traces  de  programmabilité,  c'est  à  dire  d'automation  des  opérations   de calculs, remontent aussi à l'antiquité avec le théâtre mécanique de Héron d'Alexandrie (10­70 ap J.­C.) mettant en scène une pièce de 10 minutes à l'aide d'objets animés par un système de cordes et leviers dont l'exécution était décidée à l'avance. C'est en associant ces premiers outils d'automation du calcul avec la notion de programmabilité que l'on peut constituer l'origine du sens donné à l'ordinateur moderne. On peut alors parler de d'ordinateur comme d'une machine traitant des données (calculs) à partir d'instructions données (programmabilité). En français, le terme \emph{computer} se traduisait initialement par "calculateur". C'est un professeur de latin de la Sorbonne, Jacques Perret, qui, mandaté par IBM France, a baptisé l'objet d'\emph{ordinateur} en référence à la figure de l'\emph{ordonnateur} dans la notion d'ordre ecclésiastique de la croyance catholique.

Néanmoins, ce n'est qu'entre 1940 et 1945 qu'ont surgi les premiers ordinateurs semblables à ceux que nous connaissons aujourd'hui. Il s'agissait de grandes machines occupant des pièces entières et consommant l'énergie de centaines de micro-ordinateurs actuels. La révolution informatique des années 1940 est le produit de la fusion entre divers efforts d'explorations théoriques et pratiques \citep{Black2002}. En se basant sur la \emph{tradition mathématique} consituée, entre autre, par les oeuvres de Leibniz et Boole, Alan Turing réussit à exposer les bases de l'informatique d'intérêt général dans les années 1930, avec la "Machine Universelle de Turing". Il s'agit d'une représentation abstraite et théorique d'un dispositif informatique. Selon les mots de son propre auteur, "il est possible d'inventer un machine unique pour informatiser n'importe quelle séquence"\footnote{“it is possible to invent a single machine which can be used to compute any computable sequence”} \citep[p.241]{Turing1936}. 

Cependant, l'inspiration abstraite de Turing s'est réalisée concrêtement par le fait d'une tradition distincte: la \emph{tradition d'ingenieurie}. Cet ensemble d'écoles pratiques a permis, en effet, dans les années 40, que soient construitent les premières machines élèctroniques de calcul. Ainsi, les premiers dispositifs élèctroniques incorporant le système théorique de Turing sont le résultat de la culmination des héritages empiriques d'auteurs comme John Napier (1550-1617), Blaise Pascal (1623-1666), Charles Babbage (1791-1871), entre autres.

Face à ces machines, est apparue la nécessité de les contrôler, de produire des instructions qui pourraient être prescrites. Ainsi, comme conséquence de cette nécessité, est né le troisième élément qui constitue l'informatique moderne : la \emph{tradition de programmation informatique}. Ceci s'est produit notamment dans le cadre de l'effort scientifique réalisé pendant la seconde guerre mondiale, par exemple autour de la machine ENIAC, développée par l'Université de Pennsylvanie.

Servant des intérêts belliqueux, les premières machines informatiques se dédiaient à la rapidité des calculs et, pour cela, étaient construites avec des objectifs particuliers et isolés, de telle manière que l'on entendait alors les notions, aujourd'hui établies, de Logiciel (\emph{Software}) et de Matériel (\emph{Hardware}), comme un seul et unique objet, comme fondamentalement dépendantes l'une de l'autre. Néanmoins, l'idée "d'intérêt général" qui se trouve dans le système de Turing décrivait une plate-forme compatible avec n'importe quel ensemble d'instructions, comme un dispositif générique capable de réaliser des calculs variés et adaptables. En d'autres termes, bien que les racines théoriques de l'informatique aient différencié le logiciel du matériel, les premières réalisations pratiques ont créé des machines qui ne permettaient pas à ces deux objets d'évoluer indépendamment. Ces limitations apparurent avec clarté en élargissant l'usage de ces premières machines au monde de l'entreprise. Selon les mots de Maurice Black: "La clé pour construire un ordinateur utilisable -- et, par conséquence, rentable -- ne réside pas dans le fait de le dessiner pour réaliser des fonctions ou des calculs spécifiques, mais dans le fait de construire un matériel d'intérêt général qui pourrait être programmé à réaliser n'importe quelle tâche"\footnote{“the key of making computer usable – and therefore profitable – lay not in designing them to perform specific functions or calculations, but in bulding general-purpose hardware that could easily be programmed to perform any task.”} \citep[p.40]{Black2002}.
Cependant, comme le note l'historien de l'informatique Micheal Mahoney à propos de cette période :

\begin{quote} 
Nous n'avons pratiquement aucun compte-rendu historique de comment, à partir de années 1950, les gouvernements, les entreprises et les industries ont informatisé leurs opérations. En dehors de quelques études avec une dominante sociologique sur les années 1970, la programmation, comme nouvelle activité technique, et les programmateurs, comme nouvelle force de travail, n'ont pas reçu d'attention historique.\footnote{“we have pratically no historical accounts of how, starting in the early 1950's, government, business, and industry put their operations on the computer. Aside from a few studies with a primarily sociological focus in the 1970's, programming as a new technical activity and programmers as a new labor force have received no historical attention.”} \citep[p.92]{Mahoney2002}.
\end{quote}

Bien qu'il soit difficile de préter attention aux efforts isolés qui ont permis l'abstraction progressive du logiciel comme un objet à part entière, sujet à des évolutions indépendantes de la plate-forme où il est exécuté, on peut néanmoins souligner ici l'un des faits importants de l'histoire de l'informatique, qui a permis qu'émerge le concept moderne de \emph{software}. Dans l'environnement des problématiques visant à construire un \emph{hardware} d'intérêt général, le mathématicien John Von Neumann a décrit \citep{Neumann1945} une architecture dont la prétention était d'être entièrement digitale, c'est à dire, que les instructions soient enregistrées dans la mémoire de l'ordinateur et non comme un circuit mécanique spécifique. Ainsi, les instructions pouvaient être transportées d'un ordinateur à un autre. Cette idée de "programme enregistré" a changé le visage de l'informatique moderne et a différencié le matériel du logiciel.

D'un côté, cela a permis que les instructions puissent être développées abstraitement, sans lien physique avec le matériel de la machine et, pour cela, a ouvert les possibilités d'un développement technologique collaboratif. D'un autre côté, ceci a été à l'origine du phénomène de \emph{blackboxing} (boîte noire) des programmes, permettant à ses développeurs de mettre à disposition un produit fini dont le code source serait caché. Ainsi, pour ces deux raisons principales, Maurice Black a affirmé que "l'abstraction du logiciel du matériel est l'aspect le plus important des débuts de l'histoire de la programmation"\footnote{"Software's abstraction from hardware is the single most important aspect of early programming history."} \citep[p.49]{Black2002}.

Néanmoins, le terme de logiciel a pris du temps à être utilisé par les actants des technologies informatiques. Selon Matthew Füller \citep{Fuller2008}, le premier usage public de ce mot se trouve dans un article de la revue \emph{American Mathematical Monthy} de 1958 \citep{Tukey1958}. Il s'agissait alors de l'idée selon laquelle tout les problèmes mathématiques pouvaient être résolus au sein même des mathématiques, idée qui se rencontre aujourd'hui autour du concept d'algorithme, entendu comme abstraction libérée des détails de l'implémentation. Mais ce n'est qu'en 1968, losque IBM décida, pour échapper à une accusation d'\emph{Antitrust} de l'administration américaine, de diviser sa section informatique en deux départements, \emph{Hardware} et \emph{Software}, que le terme de logiciel a pris un bonne partie de son sens commun. Néanmoins, c'est avec le propagation des ordinateurs personnels et la démocratisation de l'accès aux technologies informatiques, que l'objet "logiciel" apparaîtra dans toute sa dimension politique et sociale.

\subsection{\'Eléments principaux du matériel et du logiciel} \label{1.1.2}

Bien que sa structure ait évolué avec le temps, l'ordinateur est généralement composé des mêmes types d'éléments : une unité centrale de contrôle qui gère les différents composants, une unité arythmétique et logique qui réalise les opérations et une unité de mémoire dans laquelle les données sont enregistrées et lues. L'ordinateur -- comme un tout -- reçoit des entrées (\emph{input}) et restitue des sorties (\emph{output}). Ce couple \emph{input}/\emph{output} est reproductible à tous les niveaux de la hiérarchie qui compose un ordinateur: entrées et sorties d'un programme, d'un circuit, d'un composant, etc. L'ensemble des éléments physiques d'un ordinateur constituent son matériel (\emph{hardware}). Sa fabrication a été modifiée avec le temps, en fonction d'objectifs de rentabilité, techniques, spatiaux, qui transforment ses origines mécaniques (transistors, tubes, etc.) en circuits miniatures intégrés. Il est intéressant d'évoquer ici que les dernières recherches théoriques à propos de \emph{hardware} présentent des technologies quantiques, chimiques et optiques qui indiquent des performances sans comparaison avec les technologies actuelles les plus avançées.

Cet ensemble matériel gère des données, des programmes, des protocoles, c'est à dire, un ensemble immatériel, désigné par le terme générique de logiciel (\emph{software}). Tous les logiciels sont créés à partir de un ou plusieurs languages de programmation. Il s'agit de langages exclusivement écrits, précis et concis, afin d'éviter toute ambig\"uité dans les instructions qu'ils formulent. Une fois qu'ils sont interprétés par le \emph{hardware} (compilés), le \emph{software} apparait seulement comme une succession de bits (0 et 1), un flux binaire incompréhensible pour un être humain (Figure \ref{fig1.1}). Bien que les logiciels puissent désigner un nombre illimité d'objets, ils peuvent être classifiés en plusieurs types :

\begin{itemize}
\item Le \emph{Système Opérationnel} (SO), comme par exemple, Windows, Unix ou DOS. Il organise et coordonne l'activité et la distribution des ressources finies du matériel.
\item Les \emph{bibliothèques}. Un ensemble de "sous-routines" invoquées pour développer des programmes majeurs.
\item Les \emph{données}. Il s'agit tout autant de protocoles de transfert (SMTP, FTP, etc), que de formats de fichiers (JPEG, HTML, MPEG, etc).
\item Les \emph{applications}. Ce sont les outils bureautiques (MS Office, Open Office), Internet (navigateur, serveur web), graphique (éditeur d'image, création 3D), audio (Composition musicale, mixage), ingenierie du logiciel (compilateur, debuggeur), jeux, entre autres.
\end{itemize}

\begin{figure}[htb]
\caption{Code source et format binaire}
\label{fig1.1}
\begin{multicols}{2}
\begin{verbatim}
$ echo “Hello World”
\end{verbatim}
\columnbreak
\begin{verbatim}
0110010101100011011010000
1101111001000000010001001
0010000110010101101100011
0110001101111001000000101
0111011011110111001001101
1000110010000100010
\end{verbatim}
\end{multicols}
\end{figure}

\section{P.C. et logiciel} \label{1.2}

\subsection{le P.C. et l'informatique de masse} \label{1.2.1}

Les ordinateurs coûtaient si cher à acheter, utiliser et entretenir que seulement quelques universités, entreprises et organes gouvernementaux en possédaient. Grâce aux évolutions technologiques qui réduirent les coûts de production, l'espace et l'énergie utilisés par chaque unité, ont surgi dans les années 1970 les premiers micro-ordinateurs, ou ordinateurs personnels (\emph{Personnal Computer}, PC), que le terme ordinateur vient aujourd'hui désigner par défaut. Désignant uniquement les ordinateurs de bureau (\emph{Desktop}), récemment, ce terme désigne aussi les ordinateurs portables et les dispositifs réduits (netbook, PDA, etc). Ils sont caractérisés par le fait d'être utilisés par une seule personne à la fois, ce qui les différencie des serveurs.

Il y a un environnement spécifique autour de ces innovations technologiques. Pendant la période de révolution culturelle étatsunienne des années 1970, les campi universitaires (notamment celui de l'Université de Stanford, en Californie) sont le lieu de rencontre des grands noms de la production des PCs, parmi lesquels, on trouve, Steve Jobs et Steve Wozniac (co-fondateurs d'Apple), Bill Gates et Paul Graham (Microsoft), Paul Graham, Bill Joy, etc, qui cohabitaient avec divers mouvements sociaux: marxiste, anti-guerre au Vietnam, zen-bouddhiste, écologiste, rock, fiction scientifique, entre autres \citep{Breton1990}. Bien que ces récents acteurs des nouvelles technologies ignorent relativemment ces mouvements sociaux, ils restent influencés par une ambiance d'hyper-diversité, alternative, de transgression et d'innovation qui formèrent les bases de ceux qui allait devenir les "Pirates de la Silicon Valley"\footnote{Titre d'un film de Martin Burke (1999), inspiré par le livre : FREIBERG, Paul et SWAINE, Micheal, \emph{Fire In The Valley}, 2000.}.

D'accord avec Paul Graham, ingénieur informatique et contemporain de cette époque, qui témoigne dans son essai \emph{The Power of a Marginal} : "Le monde n'avait pas encore pris conscience que créer une compagnie d'ordinateurs était du même ordre qu'être artiste ou peintre" \citep{Graham2006}. Ainsi, en admettant l'idée que les choses nouvelles et brillantes sont inspirées par les marges d'une société, c'est la coexistence ouverte de beaucoup de marges culturelles dans un même temps et espace qui a permis l'ambiance de liberté et d'audace scientifique propice aux révolutions technologiques.

Les créateurs des ordinateurs personnels n'imaginaient pas le succès qu'auraient leurs modèles technologiques \citep{Negroponte1996}. Ils profitèrent de cette situation en bénéficiant d'innovations à moindre coût. Par exemple, la compagnie Xerox, qui a inventé l'interface graphique et la souris, a partagé ses innovations avec Apple, sans recevoir aucun dédommagement. Les dirigeants de Xerox ne croyaient pas en l'utilité de ces outils et ont permis à Apple d'apprendre leurs fonctionements en détail. Ce sont ces mêmes inventions qui ont contribué au succès de l'\emph{Apple II} en 1977 et du PC de manière générale.

Enfin, les premières productions d'ordinateurs personnels étaient réalisées par des groupes réduits, souvent en conditions précaires, ce qui a empêché certains de faire évoluer leurs structures et d'accompagner l'élargissement du marché de consommation. Cependant les grands groupes, comme IBM ou HP, n'ont pas attendu longtemps avant de rentrer dans le marché et proposer des produits équivalents. En  plus de s'étendre à un public d'ingénieurs, le PC est apparu comme un moyen d'apporter un meilleure qualité de travail \citep{Toffler1985}, et c'est dans les bureaux que le PC a été accepté à grande échelle le plus rapidemment. Le modèle de station de travail (\emph{Workstation}), considéré comme une plate-forme de haute productivité, s'est substitué petit à petit aux outils de bureau traditionnels.

\subsection{Le développement des logiciels et le début de la culture du libre}
\label{1.2.2}

Le logiciel et l'ensemble des applications qui composent un système n'attirent pas l'attention de l'industrie informatique. \`A part Microsoft, aucun actant ne développe un système opérationnel à prétention universelle, c'est à dire, cherchant à fonctionner sur n'importe quel matériel. Comme le confie le P.D.G d'Apple à propos de Bill Gates, au cours d'un entretien à l'émission \emph{All Things Digital} : "Il a créé la première compagnie de logiciels avant que quiconque dans cette industrie sache ce qu'est une compagnie de logiciels"\footnote{\emph{All Things Digital}, 30 mai 2007.}. De fait, à cette époque, l'industrie informatique cherchait principalement à vendre et entretenir du matériel. Les logiciels étaient considérés comme accessoires parce que la majorité des utilisateurs les développaient eux-mêmes. C'est l'apparition d'un système multi-tâches (\emph{multitask}) qui a imposé la forme des softwares que nous connaissons aujourd'hui, rendant possible la commercialisation de disques durs et de bandes magnétiques permettant d'enregistrer, modifier et réutiliser des programmes \citep{Muller2001}.

Une information importante est que beaucoup de systèmes partagés qui fonctionnaient sur les ordinateurs centraux des grandes institutions utilisaient la technologie Unix. Cette technologie fut développée par Ken Thompson et Dennis Ritchie au sein des laboratoires de AT\&T (Bell), et est disponible dans sa première version en 1969. Néanmoins, à cause d'une action en justice (1949-1956) pour abus de position dominante et pour éviter toute nouvelle infraction au jugement, le système Unix n'est pas commercialisé. Ce programme est mis sous licence que les institutions pouvaient acheter. Ainsi, le logiciel étant distribué sans service après-vente, ni correction de \emph{bug}, un effort actif de développement se mit en place dans les universités. Le réseau \emph{Usenet} est devenu une plate-forme d'échange d'informations et d'aide à l'utilisation d'Unix et les innovations et corrections de bug circulent rapidemment. En plus d'avoir un rôle de coordinatrice, l'Université de Stanford (Berkeley, CA-EUA) commence à développer sa propre version d'Unix - BSD (\emph{Berkely Software Distribution}) - publiée pour la première fois en 1978 par Bill Joy. Quatre ans après, en 1982, d'autres versions d'Unix sont commercialisées par IBM, HP, DEC, afin d'êtres utilisées sur leurs matériels respectifs. \`A la même date, Bill Joy quitte l'Université de Berkeley et fonde SUN dont les machines utiliseront BSD 4.2. En 1984, AT\&T réussit, après une nouvelle action en justice, à entrer dans le marché de l'informatique comme acteur à part entière et publie pour la première fois une version commerciale d'Unix dont le code-source sera dès lors fermé \citep{Muller2001}.

Unix a donc été la première proposition suffisamment aboutie ayant permis de penser un système opérationnel universel. Avec un code-source accessible, elle a profité, depuis son origine, des perfectionnements, modifications et adaptations qui ont permis une appropriation étendue, variée et créative. Au-delà de ça, l'influence de ce système opérationnel a dépassé le cadre des constructeurs de serveur. En effet, l'ancêtre de l'Internet, Arpanet, a été organisé et installé à partir de système Unix. Cette technologie, bien que propriétaire, a été appropriée par toute une génération d'ingénieurs dans une dynamique constante d'échanges, de divisions et de perfectionnements. C'est probablement là que se trouvent les racines de la philosophie \emph{open-source}, au milieu des ardeurs des révolutions culturelles et technologiques des années 1970 aux \'Etats-Unis. Il semble que l'interdiction pour AT\&T de commercialiser son produit a laissé un espace temporairement vide dans le processus de radicalisation propriétaire de l'industrie informatique et particulièrement dans l'immense marché qui allait s'ouvrir avec l'ordinateur personnel.

\section{Logiciel libre et code ouvert: communautés, licences et institutions}
\label{1.3}

\subsection{Inventer et protéger un logiciel libre}
\label{1.3.1}

C'est une petite faille dans le système de protection de Unix qui a permis, à court terme, que celui-ci soit approprié, développé et perfectionné par un ensemble d'actants de diverses localités. De cette brève période est restée une tradition  d'échange "ouvert" d'informations et, bien que déjà présente dans le milieu de l'informatique, il s'agissait pour ses adeptes, de la protéger pour que ses bénéfices ne soient pas appropriés. Cet intérêt a été rapidemment saisi depuis la permière version d'Unix développée par l'Université de Berkeley et, à cette fin, la Licence BSD a été créée. Cette déclaration contractuelle oblige les développeurs et les utilisateurs à mentionner les noms des auteurs dudit programme, elle exclue toute responsabilité en cas de dysfonctionnement, le code-source est ouvert, mais il peut-être fermé, pour une utilisation commerciale par exemple.

\'A partir de 1984, la Fondation du Logiciel Libre (FSF - \emph{Free Software Fondation}), fondée par Richard Stallman, va articuler en termes idéologiques et politiques les défis d'un logiciel qui veut être désigné de "libre". L'idée est que chaque ligne de code est la parcelle d'une information qui, étant suivie, partagée et discutée, ira mieux s'adapter. Nous pouvons observer que les métaphores ne manquent pas aux discours de la fondation, les recettes de cuisine, par exemple : en découvrant que battre des oeufs et les jeter dans une poêle permet de faire une omelette; pourquoi alors devrions-nous limiter notre liberté de partager cette astuce avec notre voisin et notre curiosité à développer des recettes dérivées ? Avec cette simple question, Richard Stallman ne veut pas défendre un logiciel gratuit, comme peut le laisser entendre le mot anglais \emph{free} ("\emph{free as free beer}"), mais plutôt un modèle de développement ("\emph{free as free society}"). Pour cela, en français comme en portugais, ce type de logiciel est dit "libre/\emph{livre}" et non "gratuit/\emph{gratuito}". De même, c'est pour cette raison que l'appellation la plus utilisée par les institutions internationales aditionnent au sigle anglais le terme \emph{libre} de la langue espagnole et française :\emph{Free/libre and Open-Source Software} (F/LOSS).

Pour servir ces objectifs, Richard Stallman écrit la Licence Publique Générale (GPL - \emph{General Public License}) qui inspire le mouvement du logiciel libre. Un code sous licence GPL est ouvert et modifiable, et doit rester ainsi. Aucune restriction à ces conditions de bases ne peut être additionnée. En complément de son intérêt juridique, la GPL véhicule une idéologie symbolisée par les "quatres libertés"\footnote{Référence aux quatres libertés proclamées par le Président des EUA, Franklin D. Roosevelt lors de son discours au congrès Américain, le 06/01/1941.} : la liberté d'éxécuter le programme, dans n'importe quel but ; la liberté d'étudier comment le programme fonctionne et de l'adapter à ses nécessités ; la liberté de le redistribuer ; la liberté de l'améliorer et de le partager avec la communauté.

Protégée par cette licence, la FSF va initier un projet ambitieux, visant à offrir un système opérationnel et des applications performantes entièrement libres. C'est ainsi que, dans le cadre du projet GNU (acronyme de \emph{GNU is Not Unix}), des centaines de programmeurs des régions développées du monde ont développé des applications semblables à celles offertes par Unix. Plusieurs des applications créées à cette époque restent jusqu'à aujourd'hui connues et performantes (notamment le compilateur GCC). En dépit de ces nombreuses années de travail, il n'est pas encore disponible un noyau (\emph{kernel}) qui permette le fonctionnement de toutes ces applications sur un système unique. En 1991, Linus Torvalds, programmeur finlandais, publie le code-source de son \emph{kernel} "Linux" qui achève l'oeuvre du projet GNU et réalise le premier système d'exploitation universel sous licence GPL: GNU/Linux.

\subsection{Logiciel Libre \emph{et/ou} ouvert} \label{1.3.2}

Alors que le mouvement du logiciel libre évolue encore dans un environnement peu connu, des voix se lèvent en son sein pour une ouverture aux partenariats avec des institutitons privées. De fait, plusieurs communautés ont proclamé que le succès de Linux n'était pas lié à sa nature "libre" mais aux avantages et à la stabilité offerts par l'accès au code-source et son modèle de développment collaboratif. En 1998, le mouvement du logiciel libre connaît une scission institutionnelle. Après 7 ans de vie, le système Linux n'était plus marginal et commençait à apparaître comme une alternative crédible pour les entreprises : Intel s'associe à Linux International, IBM construit du matériel pour être utilisé avec Linux, Netscape publie le code-source de son navigateur sur mozilla.org, les codes-sources de Staroffice (ancêtre de Open Office) et Solaris (système d'exploitation de SUN) sont, eux aussi, publiés. Beaucoup de voix se retrouvent autour d'événements comme la publication de l'essai de Eric Raymond, \emph{The Cathedral and the Bazaar} \citep{Raymond1997}, et le \emph{Open-Source Summit} réalisé par les éditions \emph{O'Reilly}. L'idée développée lors de ces évènements est d'écarter les prétentions idéologiques de la FSF et la défense du droit à la co-création/modification du code produit. C'est autour de ces idées que se crée ainsi le mouvement du logiciel \emph{open-source} (ouvert), comme philosophie pragmatique des principes qui firent le mouvement du logiciel libre, appuyée en partie par des institutions privées (IBM, Intel, O'Reilly, SUN, ReHat, etc.).

Cette dénomination de "ouvert" insiste sur l'ouverture du code-source et met de côté les engagements politiques et idéologiques, qui seront, dès lors, les revendications quasi-exclusives de la FSF et de ses projets connexes. On rassemble en général ces deux mouvements sous le sigle de \emph{Free/Libre and Open Source Software} (FOSS, F/LOSS) et en portugais, "Logiciel libre et à code ouvert" (SL/CA - \emph{Software Livre e de C\'odigo Aberto}). Pour ses propres acteurs, les noms de "mouvement \emph{open-source}" ou de mouvement "du logiciel libre" invoquent des ambitions bien différentes qui refusent d'être assimilées les unes -- idéologiques -- avec les autres -- pragmatiques. Cependant, si les défenseurs de l'\emph{open-source} font librement référence aux icônes de l'aile "libre" du mouvement, l'inverse n'est pas bienvenu, et la simple prononciation de "\emph{open-source}" sur les canaux IRC du projet GNU ou de la FSF\footnote{Canaux \#gnu \& \#fsf serveur Freenode (irc.freenode.net)} reçoit un "on ne fait pas d'\emph{open-source} ici!".

\begin{figure}[htb]

\caption{Appellations du mouvement du logiciel libre} \label{fig1.2}

\begin{center}
FOSS, F/LOSS, SL/CA\\

\begin{tabular}{|c1|p{8cm}|}
\hline
Branche politico-idéologique & Branche pragmatique \\
\hline

\emph{Free Software}, Logiciel libre & Logiciel ouvert \\

 & \emph{Open-Source Software (OSS)} \\

Exemplo de comunidades: & Exemplo de comunidades: \\

GNU & BSD, Mozilla \\
\hline

\end{tabular}
\end{center}
\end{figure}

\subsection{Licences et institutions} \label{1.3.3}

Depuis le début du mouvement FOSS, et plus encore depuis la scission explicite de 1998, se sont multipliées les licences pour protéger les travaux des communautés. Dans une certaine mesure, ces licences protègent toutes l'accès au code-source mais elles se différencient par les restrictions qu'elles apportent aux quatre libertés fondamentales du projet GNU. Si quelques uns des projets qui sont financés par des entreprises restent sous licence GPL, beaucoup d'institutions, privées comme publiques, préfèrent développer des licences propres, s'écartant alors des libertés déclarées par la FSF. Souvent, les licences spécifiques sont créées pour un projet unique, pour cela on en dénombre des centaines dont les différences sont de permettre, ou non, l'utilisation avec un logiciel propriétaire, l'accès aux modifications, la publicité, les droits particuliers du directeur du projet. Selon ces critères, voici un tableau comparatif de quelques unes des licences les plus connues\footnote{Tableau copié du livre : \emph{Tribune Libre: Ténors de l’Informatrique Libre}, editions O’Reilly, p.200.}:

\begin{figure}[htb]
\begin{center}
\caption{Comparaison entre quelques-unes des principales licences libres} \label{fig1.3}
\\
\begin{tabular}{|p{4cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
Licence & Utilisation avec un Logiciel Commercial & Accès aux modifications à tous & Publiable sous certaine conditions & Présence de droits paritculiers réservés au propriétaire de la licence \\
\hline
GPL (\emph{General Public License}) & Non & Oui & Non & Non \\
\hline
LGPL (\emph{Lesser General Public License}) & Oui & Oui & Non & Non \\
\hline
BSD (\emph{Berkeley Software Distribution}) & Oui & Non & Non & Non \\
\hline
NPL (\emph{Netscape Public License}) & Oui & Non & Non & Oui \\
\hline
Domaine public & Oui & Non & Oui & Non \\
\hline

\end{tabular}
\end{center}
\end{figure}

Certaines des licences créées pour protéger un projet spécifique sont réutilisées par des projets indépendants ; de fait, l'aura que gagnent certains projets ouverts leur donne une influence propre au sein du monde du logiciel libre. Ainsi, le projet Apache, avec plus de 50\% des parts de marché comme serveur Web, a créé une licence propre (\emph{Apache Public License}) qui a été copiée par de nombreux autres projets. Cette licence est compatible avec la GPL depuis sa version 2.0 (Janvier 2004) et est utilisée notamment par Google pour certains projets ouverts. Pour illustrer une situation différente, la fondation Mozilla (projet Firefox notamment) propose une licence incompatible avec la GPL (\emph{Mozilla Pulic License}) mais tout de même reprise par SUN (\emph{SUN Public license}).

Cette situation peut être une entrave pour le monde du développement logiciel et à sa dynamique. En effet, l'idée de la GPL est de permettre à un ensemble de codes d'être compatibles, assemblables, intégrables, assimilables... D'un côté, le fait pour un code d'être protégé par la GPL assure qu'il ne peut être reproduit/modifié seulement sous ce même règlement. Pour cela, certains ont désigné la GPL de Virus Public Général (GPV - \emph{General Public Virus})\footnote{Expression pejorative commune parmi les premières critiques faites à la (L)GPL.} car elle crée et étend un monopole unique. De l'autre côté, avec la multiplication des licences, nous sommes confrontés à de nouveaux problèmes juridiques :

\begin{quote}
Quel statut doit adopter un logiciel contenant du code ouvert par différentes licences – par exemple, un programme qui intègre à la fois du code MacOS d'Apple, X server et Mozilla pour donner une version libre de Netscape Navigator ? Si de plus, la couche utilisateur de ce nouveau programme fictif utilise la bibliothèque Qt de Troll Tech, une troisième licence entre en scène. Les trois licences entraînent des limitations différentes. \citep[p.16]{Muller2001}.
\end{quote}

Nous pouvons dire que le domaine du développement logiciel se trouve ralenti par la définition de stratégies juridiques coûteuses et souvent laborieuses. Au-delà des différences sur les externalités juridiques du modèle ouvert/libre, on trouve un processus tout aussi obscur au niveau institutionnel de ces définitions. De fait, il y a une concurrence de légitimité à définir la classification de ces licences, pour définir ce qui est libre/ouvert de ce qui ne l'est pas. La FSF maintient un répertoire actualisé\footnote{\url{http://www.gnu.org/licenses/license-list.html#SoftwareLicenses}} de toutes les licences qui se veulent libres et les classe en fonction de leur compatibilité avec les différentes versions de la GPL, c'est à dire, sa propre définition du "libre". La \emph{Open Source Initiative}, fondée par Eric Raymond en 1998, alors appuyé par l'entreprise Netscape, maintient sa propre banque de données\footnote{\url{http://www.opensource.org/licenses}}, chacune des licences étant soumise à un processus d'approbation répondant à une "définition de l'\emph{Open-Source}" soulignant l'ouverture et le modèle de développement. De la même manière, l'ONU, par l'intermédiaire de ses programmes de développement (UNDP - \emph{United Nations Developement Programs}) a fondé une institution ayant pour but de définir et développer le logiciel libre et ouvert.

Ainsi, le \emph{International Open-Source Network} (IOSN)\footnote{\url{http://www.iosn.net} Pour les discussions par rapport à la reglementation des projets libres, voir : \url{http://www.iosn.net/licensing/foss-licensing-primer/}}, qui agit particulièrement dans la région Asie-Pacifique, lui aussi définit et classe les projets et leurs licences au bénéfice d'une conception hybride (liberté/modèle de développement) tournée vers les problématiques d'inclusion/exclusion digitale et de gouvernement électronique. Les gouvernemnts nationaux, particulièrement ceux de la France, du Brésil, de la Chine et d'Israel, se concentrent sur les cas d'adoption des sytèmes ouverts dans l'administration, favorisant les critères de gratuité (réduction des coûts) et d'accès au code-source (contrôle/sécurité). En Europe, c'est le cas de \emph{Interoperable Delivery of European eGovernment Services} (IDABC) et de l'\emph{Open-Source Observatory and Repository} (OSOR). Au Brésil, les initiatives semblables étaient inexistantes avant l'an 2000 et ne restaient qu'à des niveaux locaux, comme c'est le cas des télécentres (\emph{telecentros}) qui ont privilégié peu à peu l'usage de Linux. Le Logiciel Libre est devenu une priorité au niveau fédéral après un décret présidentiel de 2003\footnote{Décret Présidentiel du 29 décembre 2003} attribuant au Comité Exécutif du Gouvernement \'Electronique (CEGE - \emph{Comitê Executivo do Governo Eletrônico})\footnote{Le CEGE a été créé le 18 octobre 2000 dans le cadre du Conseil de Gouvernement de la Présidence de la République, avec l'objectif de "formuler des politiques, établir des directrices, coordonner et articuler les actions d'implémentation du Gouvernemnt \'Electronique".} "l'implementation du logiciel libre". Dans ce cas, le logiciel libre est considéré comme un recours stratégique, une option technologique favorisant, à court et long terme, une autonomie et une appropriation indépendante des outils technologiques utilisés de manière transversale dans toutes les réalisations techniques du Gouvernement \'Electronique. Selon Tulio Vianna \citep{Vianna2006}, le logiciel libre est un moyen de garantir concrètement le droit économique au développement technologique. Ce droit souligne tout autant l'idée d'une autonomie scientifique nationale, que celle d'une démocratisation de l'accès aux nouvelles technologies. Cela serait, selon les mots de l'auteur, une arme "contre la colonisation technologique des \'Etats-Unis" et un moyen de garantir une formation plus indépendante des élites.

\subsection{Postulat de Définition} \label{1.3.4}

Nous voyons que les communautés, institutions et licences qui s'opposent quant à la définition des objectifs et des moyens du "mouvement du logiciel libre", créent une concurrence autour de la légitimité à définir ce mouvement. Pour le sociologue, il s'agit de traiter de manière systématique un objet dont la nature est fluide et évolutive, comme c'est souvent le cas à propos de mouvements sociaux, par exemple, dans les analyses du mouvement alter-globalisation [Kavada, 2003].

Dans notre cas, les analyses de réseaux proposées par Arturo Escobar \citep{Escobar2003} semblent faire sens avec la structure du bas par le haut (\emph{bottom-up system}) et le haut degré d'auto-gestion qui se trouve aux alentours du logiciel libre et de ses communautés. En considérant la métaphore de la "maille" (\emph{meshwork}) au lieu de celle de "réseau" (\emph{network}), l'auteur permet l'observation d'une grande héterogénéité au sein même du mouvement social.

De cette manière, nous pouvons inclure la plupart des contradictions soulignées par les différents actants du mouvement. En prenant 3 exemples emblématiques, ceux des communautés BSD, GNU et Mozilla, nous pouvons observer que la première - BSD - n'est pas liée à une institution, mais à une licence et permet l'utilisation commerciale et fermée de son code. Pour cela, elle est exclue de la vision du libre de la communauté GNU, laquelle est liée à une institutiton (FSF) et à une licence (GPL). Les logiciels GNU sont reconnus comme "libres" par les deux autres communautés commentées ici. La communauté Mozilla est exclue par les critères de la FSF, mais ne permet pas, néanmoins, la fermeture du code de ses produits. Du point de vue de GNU, seul GNU est libre. Du point de vue de BSD, toutes les trois sont libres. Du point de vue de Mozilla, seuls GNU et Mozilla sont libres. De plus, par rapport à la dénomination \emph{open-source} : GNU ne veut pas se considérer comme \emph{open-source}, alors que BSD et Mozilla considèrent les trois communautés comme \emph{open-source}.

Dans cet environnement d'opposition, postuler que le mouvement du logiciel libre est composé de tous les acteurs qui prétendent faire partie du monde du "libre" ou "\emph{open-source}" paraît un moyen efficace de capturer le mouvement dans toute sa complexité, restant capable de révéler les nuances identitaires des actants, lors de l'observation d'un contexte particulier.

\section{Le succès du code ouvert et ses défis contemporains} \label{1.4}

Le changement qui est intervenu en 1998 avec la scission explicite entre les mondes du libre et de l'\emph{open-source} a comme conséquence majeure l'entrée des entreprises et l'attribution de budget jusqu'alors inespéré. De telles attitudes sont des moyens importants qui sont établis par des entreprises comme IBM ou SUN pour adapter les logiciels à leurs nécessités. C'est à cette époque que vont survenir des victoires décisives du Libre, quant à son influence. Par exemple, en 1998, IBM participe au projet Apache, un serveur Web, qui montre une certaine stabilité dans son développement, alors que ses équivalents propriétaires se perdaient dans des réorientations constantes. Aujourd'hui, Apache est une des conquêtes les plus significatives du Libre, étant utilisé par la majorité des serveurs web dans le monde (70\%\footnote{Fonte: \url{http://www.securityspace.com/s_survey/data/200905/index.html}}). \`A cette époque, IBM a investi un million de dollars dans Linux pour l'installer sur ses serveurs. Le système opérationnel FreeBSD a profité des investissements de Apple, Google; d'autres projets connaissent un succès notable : DNS et BIND, Sendmail, Samba, Perl, Python, Tcl/Tk.

Nous pouvons observer que les applications libres qui ont connu du succès sont relativement "fondamentales", c'est à dire, des systèmes et des programmes pour des serveurs, protocoles de réseaux ou de fichiers, des langages de programmation, etc. Dans cet environnement, le monde du \emph{Desktop}, de l'ordinateur de bureau, de l'utilisateur final (\emph{end-user}) est  encore à conquérir. De manière générale, une machine \emph{Desktop} est composée d'un système opérationnel auquel on additionne quelques éléments essentiels. Avec 90\% des parts de marché\footnote{Fonte: \url{http://marketshare.hitslink.com/report.aspx?qprid=8}}, Microsoft détient le quasi-monopole des systèmes opérationnels utilisés sur les ordinateurs personnels (\emph{Microsoft Windows}) et réduit ainsi l'usage de Linux à une minorité d'utilisateurs avancés (0,90\%). Cependant, depuis 2005, Mark Shuttleworth finance, à travers la société de service \emph{Canonical Ltd.} (Royaume-Uni), une version de Linux (Ubuntu) ayant pour objectif d'être intuitive et dédiée aux utilisateurs communs. Cette distribution de Linux semble développer sa présence dans le monde du \emph{Desktop}, notamment en étant adoptée par de grandes institutions (Wikimedia, Amazon.com, Gendarmerie française). Open Office (OOo), alternative à Microsoft Office, est un projet ancien qui bénéficie de l'appui de SUN et se trouve par défaut dans toute les distributions de Linux, tout en étant disponible aux utilisateurs de systèmes Apple ou Microsoft. Une des plus grandes conquêtes du "libre" dans le monde du \emph{Desktop} est le navigateur Web Mozilla Firefox (22\%\footnote{Source : \url{http://marketshare.hitslink.com/report.aspx?qprid=0}} du marché), qui réussit à concurrencer son équivalent propriétaire (Internet Explorer - 66\%), en offrant plus de sécurité et d'adaptabilité.

Dans une observation plus générale et économique du phénomène de l'\emph{open-source}, on note la nécessité de trouver des modèles de financements locaux qui permettent une rétribution aux développeurs et volontaires. Le début du projet GNU décrivait la participation comme un hobby rétribué par un emploi lié au domaine de la participation volontaire. Ainsi les développeurs du système GNU/Linux utilisaient une partie de leur temps de travail comme analyste système, administrateur de réseau ou programmeur, à développer des projets qui allaient donner un retour pour les entreprises employeuses. Avec l'influence de la culture \emph{open source} à d'autres domaines, l'équilibre entre volontarisme et rétribution n'était pas aussi bien défini. Par exemple, un projet comme Wikpédia, qui fait partie des 10 sites les plus visités au monde\footnote{Source : \url{http://www.alexa.com/topsites}} n'emploie que 23 salariés\footnote{Source : \url{http://en.wikipedia.org/wiki/Wikimedia\_Foundation}}. Dans ce sens, le mouvement du logiciel libre contribue au "Travail du consommateur" \citep{Dujarier2008} faisant que l'utilisateur-développeur crée une valeur pour l'actionnaire, ceci sans coût. Dans une étude sur la nouvelle "économie collaborative" (\emph{Wikinomics}), le groupe d'étude \emph{New Paradigm} a révélé que la pièce maîtresse de ce nouvel échiquier est l'étape de contact entre le développeur et le projet auquel il va participer. Ainsi, des plates-formes offrant ou obligeant à une rétribution garantissent un modèle économique transparent, alors que des projets plus informels ne peuvent pas garantir un retour pour l'utilisateur-développeur \citep{Tapscott+Williams2008}.

L'ouverture des communautés FOSS aux investissements privés s'est réalisée au profit d'une vision pragmatique du mouvement, c'est à dire, mettant en lumière ses qualités comme modèle de développement. Les entreprises privées ont rapidemment compris leurs intérêts à favoriser la curiosité créative de leurs clients/utilisateurs, tout comme les efforts techniques des développeurs pour adapter les produits à leurs nécessités. De cette manière, le mouvement reste éloigné du concept de \emph{copyleft} et de la philosophie du projet GNU et les exemples qui suivent viennent illustrer les récentes conquêtes de l'aile pragmatique du mouvement du logiciel libre et les problèmes que cela peut engendrer.

\subsection[Le Kit de Développement Logiciel]{Le kit de développement logiciel (SDK - \emph{Software Development Kit})} \label{1.4.1}


Le SDK (\emph{Software Development Kit}) est une plate-forme de travail mise à disposition par le producteur d'une technologie pour que les développeurs puissent créer/modifier des applications. Il s'agit d'un modèle très utilisé pour divers produits: matériel, SO, Jeux. Ce modèle se trouve tout autant pour des produits libres (Qt, JAva, Android) que pour des produits propriétaires (Flash, iPhone, Microsoft Platform) et réutilise les principes caractéristiques du modèle de développement collaboratif. Qu'il offre, ou non, l'accès au code-source, le programme en question profite de l'effort collectif des utilisateurs-développeurs.

Ceci pour être une réduction forte du concept d'\emph{open source} en faveur d'une vision très rentable pour l'entreprise productrice de la technologie. De cette manière, on peut réduire au minimum le partage des informations avec la communauté tout en profitant des avantages d'une plate-forme collaborative. Ce débat s'est montré très présent lors du lancement de la plate-forme de développement pour l'iPhone d'Apple. Le SDK est téléchargeable et utilisable librement, mais toute publication de code nécessitait un enregistrement au programme officiel. \'A la suite de cette formalité, les développeurs - même étant volontaires - étaient soumis à une clause de secret professionnel. Malgré le grand succès du dispositif et de son kit de développement, les protestations ont été telles contre la politique d'Apple que l'obligation au secret a été levée, au moins entre développeurs \citep{Willis2008}.

De plus, le kit de développement de logiciel permet aux compagnies de faciliter la collaboration des utilisateurs-développeurs, en limitant comme elles le souhaitent les compensations pour la communauté. Pour cela, le SDK est une menace directe au mouvement du logiciel libre et une restriction certaine au modèle de développement \emph{open-source} en ce que ses termes, limitations et objectifs sont entièrement établis par le société émétrice.

\subsection{L'informatique dans les nuages} \label{1.4.2}

Dans le domaine de l'administration de réseaux et de la gestion des informations, deux modèles s'opposent : favoriser le client (décentraliser l'information) ou le serveur (centraliser l'information). Les détenteurs d'informations primaires - ingenieurs, développeurs, professionnels des TI - décrivent un phénomène cyclique à l'échelle de l'administration des réseaux privés (entreprises, collectivités). Cependant, à l'échelle mondiale, le phénomène semble bien plus constant. En effet, le développement progressif d'applications Web (\emph{Web-based}) tend à centraliser l'information sur quelques serveurs de multinationales. Le meilleur exemple, dans ce cas, sont les services en ligne de Google. De fait, avec un simple navigateur, un utilisateur peut utiliser, modifier et stocker ses photos (Picasa, Flickr), vidéos (Youtube), blogs (Wordpress, Blogger), textes, feuilles de calculs, présentations (Zoho, Google Documents), emails (Yahoo, Gmail), itinéraires (Google Maps), favoris et notes (Delicious, Google Note), contacts et agendas (Google Agenda), flux d'informations (Google Reader), créer et héberger ses sites (Google Sites), entre autres. Cet ensemble d'applications dépasse déjà tout ce dont peut avoir besoin un utilisateur commun connecté à Internet. Cette situation permet d'affirmer que le navigateur pourrait devenir l'unique moyen nécessaire pour accéder aux services que les ordinateurs offrent aujourd'hui. Dans ce sens :
\begin{quote}
“Je pense pour ma part que les applications sont en train de passer au modèle Web, dans lequel le navigateur devient le principal, voire l'unique, logiciel. On a, chez Mozilla, une boutade : "le système d'exploitation n'est qu'un ensemble de drivers [programmes faisant fonctionner les périphériques de l'ordinateur, comme la carte graphique] servant à faire tourner le navigateur". Dans cette approche, Windows ne vaut pas mieux que Linux ou que Mac. Il est en arrière-plan, on l'oublie. Quant à l'application bureautique, elle est "dans le nuage", sur Internet.” \citep{Nitot2008}
\end{quote}
Cette évolution est désignée par l'expression de "l'informatique dans les nuages" (\emph{Cloud Computing}) et est en passe de devenir un fait notable et stable de l'histoire de la microinformatique. De plus son avenir est encore plus assuré avec la multiplication des petits clients (\emph{thin-client}) comme les smartphone ou les netbooks, dont les capacités matérielles ne permettent pas d'accueillir des applications locales traditionnelles.

Bien que Linux ait eu un certain succès sur ces plateformes portables, le modèle d'applications \emph{web-based} menace les conquêtes déjà obsolètes du Libre en terme d'applications locales (Gimp, Open Office) en offrant des environnements intuitifs, accessible de n'importe quel endroit connecté, mais dont le code est fermé.

\subsection{Google} \label{1.4.3}

Les acteurs privés ont eu un rôle déterminant dans le développement des logiciels ouverts et c'est grâce a ces investissements que beaucoup d'entre eux ont gagné en crédibilité. Des entreprises comme IBM (Linux), SUN (Open Office, BSD, Virtual Box), Red Hat (Fedora), Canonical (Ubuntu) ont participé de cet effort en fonction de leur intérêts et stratégies respectifs, mais aucune n'a contribué au phénomène, de manière globale, comme le fait Google depuis sa création.

Créée en 1996 comme simple moteur de recherche, l'algorithme utilisé pour les recherches a donné des résultats inespérés et a permis à l'entreprise de croître rapidement et de devenir, en à peine 10 ans, un des acteurs principaux des nouvelles technologies. Grâce à un système de publicité discrète et dédiée par l'examen des informations laissées par chaque utilisateurs, l'entreprise détient maintenant un capital financier et une influence considérable. Selon la \emph{Harvard Business Review} \citep{Iven+Davenport2008}, ce sont des milliers d'expérimentations quotidiennes qui sont réalisées sur la plus grande base de données sur la manière dont les personnes cherchent, trouvent et utilisent les informations. De plus, Google détient le plus grand nombre de profils et d'informations sur les utilisateurs d'internet dans le monde, ce qui pousse l'entreprise même à se présenter de la sorte : "Chez Google, notre mission est d'organiser toute l'information du monde"\footnote{VARDA, Kenton, Protocol Buffers: Google’s data interchange format, Google code blog, 07/07/2008.}.

Une telle influence et capacité d'investissement ont beaucoup contribué au développement et à la création de nombreux projets \emph{open source}. Google libère des employés pour faire avancer certains projets clés, en plus d'organiser un grand réseau de collaboration dans lequel des étudiants du monde entier peuvent candidater pour participer à des projets ouverts aidés par l'entreprise (\emph{Google Summer of Code}). De même, certains code-sources de ses propres projets sont mis à disposition (ex : Android, Chrome).

Enfin, les opinions exprimées sur les contributions de Google à la communauté du libre sont radicalement divisées et illustrent bien les oppositions entre les adeptes de l'\emph{open source} et du \emph{free}. Pour les plus radicaux et engagés à préserver les origines éthiques et idéologiques du mouvement, "\emph{Google is devil}"\footnote{Sujet d'une liste de mails du groupe \emph{Digital Freedom Global Activists} (DFGA -- binaryfreedom.info)} (Google est le diable) et enferme le mouvement dans les compromis déjà réalisés par l'aile pragmatique du mouvement, mais, cette fois, sous le contrôle d'un monopole d'entreprise. Pour les pragmatiques du mouvement, le soutien de Google est précieux et les transgressions faites à la GPL sont déjà monnaie courante. Cependant, tous restent préoccupés en observant certaines contradictions allant dans le sens d'une stratégie de contrôle et de monopole. Du point de vue de l'entreprise, il semble que la participation inédite qu'ils offrent aux communautés ouvertes est tout autant une clé de leur succès, qu'un risque de voir cette force de travail, qu'il ne contrôle pas, disparaitre ou changer les termes de sa participation \citep{Fabernovel2008, Fabernovel2009}.
